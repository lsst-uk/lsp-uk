#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

########################################################
## Installing LSP (with Helm/Kubernetes) on Openstack ##
########################################################

## Create gateway node, give it a public ip
## Create three nodes (stv-dev-master / stv-dev-worker / stv-dev-worker-2)
## Set local machine configuration with proxies to the gatway and the cluster nodes

nano ~/.ssh/config
..

Host lsp-gateway
    User user
    IdentityFile ~/.ssh/key
    Hostname ..

Host lsp-worker
    User user
    IdentityFile ~/.ssh/key
    Hostname ..
    ProxyCommand ssh -W %h:%p lsp-gateway

Host lsp-worker-2
    User user
    IdentityFile ~/.ssh/key
    Hostname ..
    ProxyCommand ssh -W %h:%p lsp-gateway

Host lsp-master
    User user
    IdentityFile ~/.ssh/key
    Hostname ..
    ProxyCommand ssh -W %h:%p lsp-gateway
..



## On Both nodes (stv-dev-master & stv-dev-worker): 



	##---------------------------------------------------------
	## Update & install curl
	##---------------------------------------------------------
	sudo apt-get update
	sudo apt install curl



	##---------------------------------------------------------
	## Install Docker
	##---------------------------------------------------------
	sudo apt install -y docker.io
	sudo systemctl enable docker



	##---------------------------------------------------------
	## Get Kubernetes
	##---------------------------------------------------------

	curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add
	sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
	sudo apt install -y kubeadm



	##---------------------------------------------------------
	## Disable swap memory (if running) on both the nodes
	##---------------------------------------------------------
	sudo swapoff -a



##---------------------------------------------------------
## [Master Node] Initialize kubernetes on Master Node
##---------------------------------------------------------

sudo kubeadm init --pod-network-cidr=10.244.0.0/16


...

	Then you can join any number of worker nodes by running the following on each as root:

	kubeadm join 192.168.0.10:6443 --token ...  \
	    --discovery-token-ca-cert-hash sha256:...


kubeadm token create rv4pfo.16a6dqodp1vi6qhs --print-join-command --ttl=0

..


mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config



##---------------------------------------------------------
## [Worker Node] Initialize kubernetes on Worker Node
##---------------------------------------------------------

Then you can join any number of worker nodes by running the following on each as root:

sudo kubeadm join 192.168.0.10:6443 --token ...  \
	    --discovery-token-ca-cert-hash sha256:...

..
	[kubelet-start] Starting the kubelet
	[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

	This node has joined the cluster:
	* Certificate signing request was sent to apiserver and a response was received.
	* The Kubelet was informed of the new secure connection details.

	Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

..



##---------------------------------------------------------
## [Master Node] Check Nodes
##---------------------------------------------------------

sudo kubectl get nodes
NAME             STATUS     ROLES    AGE     VERSION
stv-dev-master   NotReady   master   4m55s   v1.17.2
stv-dev-worker   NotReady   <none>   81s     v1.17.2



##---------------------------------------------------------
## [Master Node] Flannel Network
##---------------------------------------------------------

sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml


##---------------------------------------------------------
## [Master Node] Get LSP project from Github
##---------------------------------------------------------

git clone https://github.com/lsst-sqre/lsp-deploy.git


##---------------------------------------------------------
## [Master Node] Set nublado configuration variables (OAuth)
##---------------------------------------------------------

pushd lsp-deploy/develop-gke/
    nano nublado-values.yaml
        ..
        fqdn: '192.41.108.30'
        oauth_client_id: '${github_client_id}'
	oauth_secret: '${github_secret}'
        ..

popd


##---------------------------------------------------------
## [Master Node]  Install Helm (version 2.9.1)
##---------------------------------------------------------
nano ~/install_helm.sh 

..

#!/usr/bin/env bash
set -euo pipefail

if [ $# -lt 1 ]; then
    echo "usage: $0 VERSION" 1>&2
    exit 1
fi
version=$1

curl -fSL https://storage.googleapis.com/kubernetes-helm/helm-v${version}-linux-amd64.tar.gz | tar xz -C /usr/local/bin/ --strip-components=1 linux-amd64/helm


..


pushd ~/
    chmod +x ~/install_helm.sh
    sudo ./install_helm.sh 2.9.1
popd



helm version
  > Client: &version.Version{SemVer:"v2.9.1", GitCommit:"20adb27c7c5868466912eebdf6664e7390ebe710", GitTreeState:"clean"}


##---------------------------------------------------------
## [Master Node]  Install LSP
##---------------------------------------------------------

pushd lsp-deploy/develop-gke/

   ## Create Certificates 
  sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/apache-selfsigned.key -out /etc/ssl/certs/apache-selfsigned.crt
  sudo scp -r /etc/ssl/private/ . 
  sudo scp -r /etc/ssl/certs/apache-selfsigned.crt private/ 
  sudo chown ubuntu:root private/


  ## Do we need these?
  kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller


  helm init --service-account tiller --override spec.selector.matchLabels.'name'='tiller',spec.selector.matchLabels.'app'='helm' --output yaml | sed 's@apiVersion: extensions/v1beta1@apiVersion: apps/v1@' | kubectl apply -f -

   ## Install Tiller
   sudo ./install_tiller.sh

   ## Install ingress
   sudo ./install_ingress.sh private/apache-selfsigned.crt private/apache-selfsigned.key 

   ## Set Public IP
   sudo ./public_ip.sh
   .. 
	Retrieving public IP of your LSST Science Platform...
	Make sure your DNS points to this IP address:
   ..

   ## Maybe we need to set the IP manually in public_ip.sh

   sudo ./install_lsp.sh
     ..
	Installing Landing Page...
	+ helm install lsstsqre/landing-page --name landing-page --namespace landing-page
	Error: unable to recognize "": no matches for kind "Deployment" in version "extensions/v1beta1"
	ubuntu@stv-dev-master:~/lsp-deploy/develop-gke$ cat install_lsp.sh 
    .. 

   sudo ./uninstall_lsp.sh

   ## Had to delete from k8s Dashboard

   ## Let's install all the services individually
   ## Note: We can also grab the templates from ~/.helm/cache/archive/ and install from those, if we want to edit the configuration


   ## sudo helm install lsstsqre/fileserver --name fileserver --namespace fileserver

	NAME:   fileserver
	LAST DEPLOYED: Tue Feb 11 20:31:35 2020
	NAMESPACE: fileserver
	STATUS: DEPLOYED

	RESOURCES:
	==> v1/Pod(related)
	NAME                         READY  STATUS   RESTARTS  AGE
	fileserver-68f8cbd485-4v2xr  0/1    Pending  0         0s

	==> v1beta1/StorageClass
	NAME  PROVISIONER           AGE
	fast  kubernetes.io/gce-pd  1s

	==> v1/PersistentVolumeClaim
	NAME                STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE
	fileserver-physpvc  Pending  fast    1s

	==> v1/Service
	NAME        TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)                     AGE
	fileserver  ClusterIP  10.102.237.37  <none>       2049/TCP,20048/TCP,111/TCP  1s

	==> v1/Deployment
	NAME        DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
	fileserver  1        1        1           0          0s


   ## Install TAP
   sudo helm install lsstsqre/cadc-tap --name tap --namespace tap
	NAME:   tap
	LAST DEPLOYED: Tue Feb 11 20:18:06 2020
	NAMESPACE: tap
	STATUS: DEPLOYED

	RESOURCES:
	==> v1/Secret
	NAME                  TYPE    DATA  AGE
	tap-cadc-tap-secrets  Opaque  2     1s

	==> v1/Service
	NAME                        TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE
	qserv-master01              ClusterIP  10.108.143.195  <none>       4040/TCP  1s
	tap-cadc-tap-tap-schema-db  ClusterIP  10.104.58.194   <none>       3306/TCP  1s
	tap-cadc-tap-tap-server     ClusterIP  10.107.211.225  <none>       80/TCP    1s
	tap-cadc-tap-uws-db         ClusterIP  10.107.5.242    <none>       5432/TCP  1s

	==> v1/Deployment
	NAME                        DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
	tap-cadc-tap-mock-qserv     1        1        1           0          0s
	tap-cadc-tap-querymonkey    0        0        0           0          0s
	tap-cadc-tap-tap-server     1        1        1           0          0s
	tap-cadc-tap-tap-schema-db  1        1        1           0          0s
	tap-cadc-tap-uws-db         1        1        1           0          0s

	==> v1beta1/Ingress
	NAME                                HOSTS  ADDRESS  PORTS  AGE
	tap-cadc-tap-anonymous-ingress      *      80       0s
	tap-cadc-tap-authenticated-ingress  *      80       0s

	==> v1/Pod(related)
	NAME                                         READY  STATUS             RESTARTS  AGE
	tap-cadc-tap-mock-qserv-85d5b5fd8-krwh7      0/1    ContainerCreating  0         0s
	tap-cadc-tap-tap-server-7b546c5b4f-6jz65     0/1    ContainerCreating  0         0s
	tap-cadc-tap-tap-schema-db-7f454d4587-hbxl8  0/1    ContainerCreating  0         0s
	tap-cadc-tap-uws-db-7bd577cdcd-g6bcx         0/1    ContainerCreating  0         0s


   ## Install Firefly
   sudo helm install lsstsqre/firefly --name firefly --namespace firefly

	NAME:   firefly
	LAST DEPLOYED: Tue Feb 11 20:24:04 2020
	NAMESPACE: firefly
	STATUS: DEPLOYED

	RESOURCES:
	==> v1/Deployment
	NAME           DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
	firefly        1        1        1           0          1s
	firefly-redis  1        1        1           0          1s

	==> v1beta1/Ingress
	NAME     HOSTS  ADDRESS  PORTS  AGE
	firefly  *      80       1s

	==> v1/Pod(related)
	NAME                            READY  STATUS             RESTARTS  AGE
	firefly-7f4fd5765-s5q6l         0/1    Pending            0         1s
	firefly-redis-75c96dc865-s6sq5  0/1    ContainerCreating  0         1s

	==> v1/Secret
	NAME            TYPE    DATA  AGE
	firefly-secret  Opaque  1     1s

	==> v1/Service
	NAME           TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)   AGE
	firefly-redis  ClusterIP  10.96.160.166  <none>       6379/TCP  1s
	firefly        ClusterIP  10.104.48.146  <none>       8080/TCP  1s


    ## Install Nublado
    sudo helm install lsstsqre/nublado --name nublado --namespace nublado --values nublado-values.yaml

	NAME:   nublado
	LAST DEPLOYED: Tue Feb 11 20:25:11 2020
	NAMESPACE: nublado
	STATUS: DEPLOYED

	RESOURCES:
	==> v1/Pod(related)
	NAME                             READY  STATUS             RESTARTS  AGE
	nublado-hub-5c8cbcd864-cxfcz     0/1    Pending            0         1s
	nublado-proxy-7b85bf54bc-fmdv5   0/1    ContainerCreating  0         1s
	nublado-wf-api-5844f88788-9mw8r  0/1    ContainerCreating  0         1s

	==> v1/ConfigMap
	NAME                       DATA  AGE
	nublado-fs-mounts          1     2s
	nublado-jupyterhub-config  1     1s
	nublado-jwt-cert           1     1s
	nublado-resourcemap        1     1s

	==> v1/ServiceAccount
	NAME               SECRETS  AGE
	nublado-dask       1        1s
	nublado-hub        1        1s
	nublado-prepuller  1        1s

	==> v1/ClusterRoleBinding
	NAME               AGE
	nublado-hub        1s
	nublado-prepuller  1s

	==> v1/Role
	NAME               AGE
	nublado-dask       1s
	nublado-hub        1s
	nublado-prepuller  1s

	==> v1/RoleBinding
	NAME               AGE
	nublado-dask       1s
	nublado-hub        1s
	nublado-prepuller  1s

	==> v1/Service
	NAME            TYPE      CLUSTER-IP      EXTERNAL-IP  PORT(S)                        AGE
	nublado-hub     NodePort  10.103.99.205   <none>       8081:31574/TCP                 1s
	nublado-proxy   NodePort  10.102.241.203  <none>       8000:31307/TCP,8001:32761/TCP  1s
	nublado-wf-api  NodePort  10.111.159.84   <none>       8080:30585/TCP                 1s

	==> v1/Secret
	NAME         TYPE    DATA  AGE
	nublado-hub  Opaque  10    2s

	==> v1/PersistentVolumeClaim
	NAME                 STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE
	nublado-hub-physpvc  Pending  1s

	==> v1/ClusterRole
	NAME               AGE
	nublado-hub        1s
	nublado-prepuller  1s

	==> v1/Deployment
	NAME            DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
	nublado-hub     1        1        1           0          1s
	nublado-proxy   1        1        1           0          1s
	nublado-wf-api  1        1        1           0          1s

	==> v1beta1/CronJob
	NAME               SCHEDULE    SUSPEND  ACTIVE  LAST SCHEDULE  AGE
	nublado-prepuller  35 * * * *  False    0       <none>         1s

	==> v1beta1/Ingress
	NAME            HOSTS  ADDRESS  PORTS  AGE
	nublado-proxy   *      80       1s
	nublado-wf-api  *      80       1s



popd


helm ls --all
NAME      	REVISION	UPDATED                 	STATUS  	CHART               	NAMESPACE
fileserver	1       	Wed Dec  4 13:56:13 2019	DEPLOYED	fileserver-0.2.1    	default  
firefly   	1       	Wed Dec  4 13:52:19 2019	DEPLOYED	firefly-0.1.0       	firefly  
nginx     	1       	Wed Dec  4 12:39:07 2019	DEPLOYED	nginx-ingress-1.26.1	nginx    
nublado   	1       	Wed Dec  4 13:49:36 2019	DEPLOYED	nublado-0.3.5       	nublado  
tap       	1       	Wed Dec  4 13:52:16 2019	DEPLOYED	cadc-tap-0.1.2      	tap  


##---------------------------------------------------------
## [Master Node]  Deploy Kubernetes Dashboard
##---------------------------------------------------------
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml



##---------------------------------------------------------
## [Local Machine] Kubernetes Dashboard Web GUI 
##---------------------------------------------------------
ssh -L 8080:127.0.0.1:8001 lsp-master



##---------------------------------------------------------
## [Master Node] If we want to create Admin user (Probably dont need this step)
##---------------------------------------------------------
nano dashboard-adminuser.yaml

..

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard



kubectl apply -f dashboard-adminuser.yaml
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')



##---------------------------------------------------------
## [Master Node] Start Dashboard Proxy
##---------------------------------------------------------
sudo kubectl proxy
 > Starting to serve on 127.0.0.1:8001


##---------------------------------------------------------
## [Local Machine] Tunnel Connection to Dashboard
##---------------------------------------------------------

ssh -L 8001:127.0.0.1:8080 lsp-master

# Go to: 
# http://localhost:8080/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login
# Use "tiller" token


kubectl -n kube-system describe secret 

	..
	Name:         tiller-token-rwc98
	Namespace:    kube-system
	Labels:       <none>
	Annotations:  kubernetes.io/service-account.name: tiller
		      kubernetes.io/service-account.uid: 

	Type:  kubernetes.io/service-account-token

	Data
	====
	ca.crt:     1025 bytes
	namespace:  11 bytes
	token:      eyJhbGc  ..  UgW6A


##---------------------------------------------------------
## Check Deployments & Services
##---------------------------------------------------------

# A number of deployments did not start successfully
 


## [Local Machine] Tunnel Connection to Firefly/Nublado/TAP

pod_ip=
ssh -L 8081:${pod_ip:?}:8080 lsp-master


# Go to: 
# http://localhost:8081/suit
# Not sure why "suit"




## Nublado & Fileserver not working, probably because they require an NFS or similar storage system.

## Let's create a temp localhost claim (PVC) & storage as described here:
   https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/

## We can now use these, and redeploy the nublado helm chart, by replacing any references to the nublado (fileserver) storage claims, with the claims created here
## For example see the charts/nublado folder 



